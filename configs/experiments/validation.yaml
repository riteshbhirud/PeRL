# Validation Experiment (Session 3)
# Purpose: Verify 512-step insights generalize to full training
# 1 experiment: DoRA at 1024 steps
# Time: ~1.5 days on 4x A100

base:
  model:
    model_name_or_path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    attn_implementation: "flash_attention_2"
    dtype: "bfloat16"

  dataset:
    dataset_name_or_path: "open-r1/DAPO-Math-17k-Processed"

  training:
    max_steps: 1024  # Full length validation
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 16
    learning_rate: 1.0e-5
    warmup_ratio: 0.1
    save_strategy: "steps"
    save_steps: 256
    logging_steps: 10
    use_liger_kernel: false
    max_completion_length: 2048
    num_generations: 2
    max_prompt_length: 512
    use_vllm: true
    vllm_mode: "colocate"
    vllm_gpu_memory_utilization: 0.3
    loss_type: "dapo"
    lr_scheduler_type: "cosine"
    report_to:
      - wandb

  peft:
    use_peft: true
    task_type: "CAUSAL_LM"
    r: 32
    lora_alpha: 64
    lora_dropout: 0.05
    target_modules:
      - q_proj
      - v_proj
      - k_proj
      - o_proj
      - up_proj
      - down_proj
      - gate_proj

  tracker:
    enable_spectral_tracking: true
    enable_gradient_tracking: true
    spectral_log_frequency: 100
    gradient_log_frequency: 100

  wandb:
    use_wandb: true
    project: "peft-rlvr-mechanistic"
    tags:
      - validation
      - full_length

  common:
    seed: 42
    output_dir: "output/validation"

experiments:
  - name: "dora_validation_1024"
    peft:
      type: "dora"
    wandb:
      tags:
        - validation
        - dora
        - full_length
        - 1024_steps
